---
title: "Environmental sensor data management and analysis with R"
author: 
- "Alexander G. MacPhail, Marcus Becker, Elly C. Knight"
date: "`r Sys.Date()`"
output:
  bookdown::html_document2: default
  pdf_document: default
description: "wildRtrax is for users who want to create full-cycle work flows for their environmental sensor data from either autonomous recording units (ARUs) or remote camera traps"
github-repo: ABbiodiversity/wildRtrax
cover-image: "/book/images/cover.png"
url: https://abbiodiversity.github.io/wildRtrax
link-citations: yes
graphics: yes
site: bookdown::bookdown_site
header-includes:
- \usepackage{graphicx}
- \usepackage{float}
- \usepakcage{wallpaper}
urlcolor: blue
---

![](assets/cover.png)

```{r include=FALSE}
# Include initial packages
library(tinytex)
library(kableExtra)

# Automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

```{r include=FALSE}
# Knitr setup for markdown chunks
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(engine.opts = list(bash = "-l"))
```

# Preface {-}

`wildRtrax` (pronounced *‘wilder tracks’*) is an R package to help ecologists create full-cycle environmental sensor data work flows. 

## Who is this book for?

This book is a valuable resource for anyone working with environmental sensor data and seeking to manage it using R. It is suitable for both experienced users and those new to the subject, as it is written in a clear and accessible style. While the book is designed to be user-friendly, it also contains advanced concepts and functions that may require additional reading to fully understand. However, the authors encourage readers to ask questions and provide feedback, as the book and workflows will continue to evolve.

## How should I read this book?

1. Data pre-processing
2. Data analysis

## Why in R? Why bookdown?

Bookdown is a powerful tool for writing and publishing books, especially those that require complex formatting or incorporate code and data analysis and integrates well with RStudio.

## Impact and intentions



## Acknowledgements

* The [Alberta Biodiversity Monitoring Institute](https://abmi.ca) supports and funds all operations surrounding `wildRtrax` and [WildTrax](https://www.wildtrax.ca).
* Many thanks to [Dr. Richard Hedley](https://richardwhedley.wordpress.com/) for providing the basis for the `wt_wac_info` internal function for *wac* file support


<!--chapter:end:index.Rmd-->

# Introduction

## Environmental sensors

Environmental sensors are devices used to measure and record various environmental parameters, such as temperature, humidity, pressure, light, and sound. These sensors can be used to monitor changes in the environment over time and provide valuable information about the conditions that wildlife species experience in their habitats.

In wildlife ecology, environmental sensors are commonly used to study the behavior, habitat preferences, and distribution of wildlife species. For example, temperature sensors can be used to monitor the microclimate of an animal's habitat and determine the effects of temperature on its behavior and physiology. Humidity sensors can be used to study the impact of moisture on the distribution and abundance of species in different ecosystems. Sound sensors, principally, autonomous recording units (or ARUs) can be used to detect and identify the vocalizations of animals, allowing researchers to study communication patterns, behavior, population size, density, habitat preference, movement and many other biological metrics. 

Environmental sensors can also be used to monitor changes in the environment caused by human activities, such as deforestation, pollution, and climate change. By measuring and recording changes in environmental conditions, researchers can better understand the impact of these activities on wildlife species and develop effective conservation strategies to protect them.

## Open data

Open data refers to data that is freely available for anyone to access, use, and share. Open data is important in the context of environmental sensors because it allows researchers, scientists, and policymakers to access and use data collected by sensors to gain insights into environmental conditions and make informed decisions.

Environmental sensors generate large amounts of data on various environmental parameters such as audio recordings and images. By making this data openly available, researchers can analyze it to gain insights into patterns and trends over time, leading to a better understanding of environmental conditions and the impact of human activities on the environment. Open data also promotes transparency and accountability in environmental research. It allows the public to access and understand the data used to inform decisions about environmental management and conservation. Additionally, open data can lead to the development of innovative solutions and technologies that can help address environmental challenges. 

Open data also helps promote reproducibility ensuring independent verification of the data. Reproducibility ensures that the methods used to collect and analyze the data are transparent and can be independently verified by other researchers, ensuring that the results are reliable and accurate, establishing scientific validity. 

## Standardizing information pipelines

Standardized workflows can help with environmental sensor data by ensuring that data is collected, processed, and analyzed in a consistent and transparent manner. This helps to ensure that the data is reliable and accurate, and can be easily compared and combined with other data sets. Standardized workflows typically involve a series of steps that are designed to ensure that data is collected and analyzed consistently across different research projects and sites. These steps may include:

* **Pre-processing**: Cleaning and filtering the raw sensor data to remove any noise or errors.
* **Quality control**: Checking the data for outliers or errors, and making corrections or adjustments as necessary.
* **Processing**: Transforming and extracting species from the raw sensor data
* **Verification**: Ensuring the species detections are correct
* **Analysis**: This step involves applying statistical or other analytical methods to the data to extract meaningful insights or calculate biological metrics.
* **Reporting**: Presenting the results of the analysis in a clear and understandable format, such as charts, graphs or tabular data. 

By standardizing these workflows, researchers can ensure that data is collected and analyzed consistently across different projects and organizations, reducing the risk of errors or inconsistencies. This helps to ensure that the data is reliable and accurate, and can be used to inform decisions related to environmental management and conservation. In addition, standardized workflows can facilitate data sharing and collaboration among researchers, as it ensures that data is collected and processed in a manner that is consistent with best practices and widely accepted standards. This can help to promote data integration and interoperability, enabling researchers to combine data from different sources to gain a more comprehensive understanding of environmental conditions and the impact of human activities on the environment.

## What is **WildTrax**?

[**WildTrax**](https://www.wildtrax.ca) is a web-enabled portal designed to manage, store, process, share and discover environmental sensor data and the  **biological data** extracted from the media. WildTrax was developed by the [Alberta Biodiversity Monitoring Institute](https://abmi.ca). `wildRtrax` serves as a parallel design and indicator to WildTrax for future analytics and functionalities.

## Why did you build `wildRtrax`?

By outlining a standardized and harmonized procedure for data intake, quality control, processing and verification of acoustic data from autonomous recording units (ARUs), `wildRtrax` and WildTrax hope to provide open work flows for using ARUs to answer meaningful biological questions in order to inform conservation and decision-making.

## Installing the package

You can install `wildRtrax` directly from this repository with:

```r
# install.packages("remotes")
remotes::install_github("ABbiodiversity/wildRtrax")
```

Or the development version with:

```r
remotes::install_github("ABbiodiversity/wildRtrax@development")
```

## Usage

All functions begin with a `wt_*` prefix. Column names and metadata align with the WildTrax infrastructure. The goal is to follow the work flow of pre-processing, linking with WildTrax, download and analysis. 


<!--chapter:end:01-Introduction.Rmd-->

# Data pre-processing

## Acoustic data basics

After an ARU is retrieved from the field, it is crucial to promptly download and secure the data to create a redundant backup. Having redundant backups not only protects against data loss but also ensures business continuity and minimizes downtime in the event of a system failure or disaster. In case there is an issue with the SD card copy during the quality control process, having a redundant backup ensures that a copy of the data can be restored.

## Acoustic data pre-processing



### Reading, filtering and selecting audio recordings

:::: {.dangerbox data-latex=""}
::: {.left data-latex=""}
Note, if you're not an R user you can stop here and upload all of your recordings directly to the organization on WildTrax. WildTrax however does not contain all the necessary data quality control measures to uptake data that are included in ``wildRtrax``
:::
::::

- Go to the organization on WildTrax
- Go to Manage > Upload Recordings to Organization
- Follow the steps in WildTrax to finalize data upload

```{r, include = T, eval=F, warning=F, message=F}


wt_audio_scanner()


```

You can choose `extra_cols = T` you'll be supplied with additional columns that can be used to help select recordings. 

```{r, include = T, eval=F, warning=F, message=F}


wt_audio_scanner()


```

It's possible that the recordings you have are longer than the maximum length WildTrax can currently import (1800 seconds or 320MB). In this case, you can use `wt_chop` to create intervals of recordings

```{r, include = T, eval=F, warning=F, message=F}


wt_chop()


```

The *modulo recording* refers to the remaining portion of the recording that is left over after it has been divided into equal intervals of the chosen duration. For example, if a recording is 120 seconds long and it is split into 50-second intervals, the modulo recording would be the final 20 seconds of the recording that does not fit into the final interval (e.g. 50 - 50 - 20). However, if the chosen interval duration is a factor of the total duration of the recording, there will be no modulo recording. For example, if a 120-second recording is divided into 60-second intervals, there will be no modulo recording since 60 seconds is a factor of 120 seconds.

At this stage, you can filter recordings from the `wt_audio_scanner` tibble output in order to select the files you're interested in. Here's an example of the ABMI's Stratified Sampling Design for the Ecosystem Health Monitoring program that's used to pick recordings across a breadth of dates and times in order to maximize the species inventory collected at a location.  

```{r, eval = F, warning = F, message = F, include = T}


abmi_blocks <- as_tibble(data.frame(julian = 90:210) %>%
                     crossing(time_index = 1:4) %>%
                     mutate(blocks = case_when(julian %in% 90:139 & time_index == 1 ~ 9,
                                               julian %in% 140:159 & time_index == 1 ~ 10,
                                               julian %in% 160:179 & time_index == 1 ~ 11,
                                               julian %in% 180:210 & time_index == 1 ~ 12,
                                               julian %in% 90:104 & time_index == 3 ~ 1,
                                               julian %in% 105:119 & time_index == 4 ~ 2,
                                               julian %in% 120:139 & time_index == 3 ~ 3,
                                               julian %in% 140:149 & time_index == 3 ~ 4,
                                               julian %in% 150:159 & time_index == 4 ~ 5,
                                               julian %in% 160:169 & time_index == 3 ~ 6,
                                               julian %in% 170:179 & time_index == 4 ~ 7,
                                               julian %in% 180:210 & time_index == 4 ~ 8,
                                               TRUE ~ NA_real_),
                            recs = case_when(blocks %in% c(4:7) ~ 180,
                                             TRUE ~ 60)))

head(abmi_blocks)


```

The Before-After Dose-Response program is another example of a sampling design. Here, we select four recordings near dawn and one recording at dusk for each location between Julian date 140 and 210, for a total of 15 audio minutes. Again, this approach allows users to maximize species diversity while accounting for processing effort.

```{r, eval = F, warning = F, message = F, include = T}

bz <- b %>%
  select(file_name:year) %>%
  inner_join(., locs, by = c("location")) %>%
  mutate(recording_date_time = force_tz(recording_date_time, "US/Mountain")) %>%
  rowwise() %>%
  mutate(angle = pull(suncalc::getSunlightPosition(date = recording_date_time, lat = latitude, lon = longitude, keep = c("altitude"))) * (180/pi)) %>%
  ungroup() %>%
  filter(between(angle,-6,6), # Take recordings between civil twilight and 6 degrees after dawn
         between(julian,140,210)) %>%
  mutate(hour = hour(recording_date_time))

# Sample on dusk recording
bz1 <- bz %>% filter(hour %in% c(20:22)) %>%
  sample_n(1, replace = F)
# And four dawn recordings
bz2 <- bz %>% filter(hour %in% c(4:7)) %>%
  sample_n(4, replace = F)

bzz <- bind_rows(bz1, bz2)

head(bzz)

```

Another example of selecting files may be geographic. Here's an example with selecting recordings with Bird Conservation Regions. 

```{r, include=T, eval=F, message=F, warning=F}

library(tidyverse)
library(sf)
library(fs)

locs <- dir_ls(path = "/users/alexandremacphail/desktop/2023MASTER/recs",regexp="*.csv") %>%
  map_dfr(~read_csv(.)) %>%
  filter(!is.na(latitude), !is.na(longitude)) %>%
  select(location, recordingDate, latitude, longitude) %>%
  distinct() %>%
  mutate(index = row_number())

locsset <- st_as_sf(locs %>% select(location, latitude, longitude) %>% distinct(), coords = c("longitude","latitude"))
locsset <- st_make_valid(locsset)
locsset <- st_set_crs(locsset, 4269)

bcrs <- read_sf("/users/alexandremacphail/desktop/BCR_Terrestrial/BCR_Terrestrial_master.shp")
bcrs <- st_make_valid(bcrs)

ggplot(bcrs_select) +
  coord_sf(crs = 4269) +
  geom_sf(aes(fill = BCRNAME)) +
  guides(fill = "none") +
  theme_bw() +
  theme(legend.position = "none") +
  scale_fill_viridis_d()

out <- st_intersection(locsset, bcrs) %>%
  as_tibble()

locss <- locs %>%
  inner_join(., out, by = c("location" = "location"))

```

In the end, how recordings are selected is completely dependent on your study design.

### Using acoustic indices and LDFCs

*Acoustic indices* and *long-duration false-colour (LDFC)* spectrograms are two tools used in the analysis of acoustic data developed by the Towsey lab at QUT. Acoustic indices are numerical measurements that quantify various aspects of sound using different measurements that reflect the soundscpe. They are used to identify and track patterns of vocal activity over time, generate soundscape and monitor ecosystem-level patterns and changes. Some commonly known acoustic indices include the Acoustic Complexity Index (ACI), which measures the diversity of sound frequencies and amplitudes, and the Acoustic Diversity Index (ADI), which measures the number and relative abundance of different vocalizations. Long-duration false-color spectrograms are visual representations of acoustic data using index values rather than spectral generation in order to provide detailed views of sound patterns over long periods of time. The use of false-colours allows for easier interpretation of the spectrogram, as different colors can be used to represent different types of sounds. These unique spectrograms are particularly useful in studies of animal vocalizations, as they allow researchers to identify specific vocalizations and track patterns of vocal activity over long periods of time, and determine seasonal phenology patterns. 

``wildRtrax`` utilizes the results of the AnalysisPrograms software which generates the csv, json and png files associated to the results, and generates tidy versions of the data that can be joined to the media in order to select recordings based on index values and patterns. Let us demonstrate some different examples while introducing the `wt_run_ap` and `wt_glean_ap` functions.

```{r, include = T, eval=F, warning=F, message=F}


wt_run_ap()


```

```{r, include = T, eval=F, warning=F, message=F}


wt_glean_ap()


```

#### Abiotic, geophonic and anthropophonic signal detection



#### Biophonic signal detection

Let's look at the entire seasonal phenology of an ABMI Ecosystem Health site

```{r, eval=F, include=T, message=F, warning=F}

ran_ap

```

While acoustic indices are good at detecting large scale patterns, it is more difficult to detect rare and elusive signals, especially those that exceed the signal-to-noise threshold to capture the signal. Neverthless, we can look at relative influence plots in order to determine whether or not a signal is occuring over time. 

```{r, eval=F, include=T, message=F, warning=F}

relative_influence

```

### Linking acoustic data to WildTrax

Wildlife Acoustics offers advanced tools for analyzing wildlife vocalizations. Two of their most popular programs are Songscope and Kaleidoscope that allows users to visualize and identify animal songs and calls and also create and run automated classifiers. The output files from these programs can be transformed into WildTrax tags to be uploaded for verification purposes. This is also the primary mechansim that ultrasonic data makes its way into the system if it is being analyzed in Kaleidoscope. 

#### Sonscope results

```{r, include = T, eval=F, warning=F, message=F}

wt_songscope_tags()

```

#### Kaleidoscope results

```{r, include = T, eval=F, warning=F, message=F}

wt_kaleidoscope_tags()

```

### Making tasks

![](assets/task.png)


```{r, include = T, eval=F, warning=F, message=F}

wt_make_aru_tasks()

```


## Image data pre-processing


## Other convenience functions

```{r, include = T, eval=F, warning=F, message=F}


wt_location_distances()


```

<!--chapter:end:02-Data-pre-processing.Rmd-->

# Acoustic data analysis


## Authorizing and downloading data from WildTrax

```{r, include = T, eval=F, warning=F, message=F}


wt_auth()


wt_get_download_summary()



wt_download_report()


```

## Wrangling data for analysis

### Species identification and observer error

Avian count surveys rely on skilled observers to identify species and estimate numbers of individuals using distinct visual or acoustic signals. Since birds rely on acoustic cues to attract mates, defend territory and communicate with one another, humans can take advantage of these unique signatures in order to identify species and individuals. In passive field-based surveys (i.e. "point counts"), human observers use distance and time-interval sampling in order for analysts to incorporate perceptibility and dectectability offsets (Buckland et al. 2001, Farnsworth et al. 2002, Solymos et al. 2013) to accurately calculate meaningful biological metrics such as population size or density which are then are used to make conservation decisions. The accuracy and reliability of these results must be high otherwise incorrect conclusions and flawed decision-making can lead to the improper management of a species or landscape.

Since the advent of affordable autonomous recording unit (ARU) technology, single-visit human-based surveys have been rapidly replaced by archiveable acoustic recordings of the environment. This has established both the fields of bioacoustics and ecoacoustics, launching ARUs as the new standard for bird population monitoring. The comparison of human-based surveys and acoustic recording technology for monitoring avian populations is well-documented (Shonfield et al. 2017, Van Wilgenburg et al. 2017, Venier et al. 2017, Pankratz et al. 2017, Yip et al. 2019). Factors such as distance from the observer or ARU (Yip et al. 2016, Knight et al. 2020, Van Wilgenburg et al. 2017), frequency range and habitat type (Yip et al. 2016) all play an important role in whether a species is detected (true positive; *TP*) or not (true negative; *TN*). Researchers can leverage the acoustic recording data over point counts in order to improve data processing quality and benefit from long-term secure data storage. Ensuring the proper integration of traditional human-collected and acoustic data [REFERENCE] is key to avian monitoring management implications and conservation goals. By creating open data sets, (Docherty et al. 2021) researchers can also merge data sets together allowing for larger coproduced (Westwood et al. 2020) and reproducible analyses.

False positives (*FP*) and false negatives (*FN*) in avian data sets are rarely reported in the scientific literature suggesting that they are either flawlessly collected or species are identified to a high degree of accuracy and / or precision. *FP*s can increase the count and therefore skew the population estimates for certain species. Morrison (2016) proposes that to avoid observer error in plant identification, additional training including active feedback approaches, continual evaluation and calibration among a multi-observer set are important. Similarly, Dennett and Nielsen (2019) determined that abundance and scale of the survey were the most influential factors in detecting rare species of plants. These types of assessments would be operationally difficult or costly to implement for field-based avian surveys given the mobile nature of birds, however utilizing the ability to return to acoustic recording would show the potential to determine the error in the avian count data. Standardizing multiple observers is known to be effective for correcting inter-observer variability in species detections in a monitoring program [REFERENCE]. While acoustic data excludes any visual cues for species identification, the principal advantage of collecting avian data with ARUs is that a permanent copy of the recording can be kept for future reference.

A standardized system of reporting, assessing and determining actions to resolve avian identification error is crucial to ensure that reporting meets a high data quality standard and that it can be used comfortably in aggregation with other data to answer a wider range of scientific questions. The goals were to provide a standard way of assessing the prevalence of false positives, false negatives and true negatives in avian datasets, to provide solutions to correct and integrate identification error into analyses, and to provide recommendations for the evaluation, training and development of skilled observers. 

```{r, eval = F, include = T, warning = F, message = F}

# Omit any abiotic data
abiotic_codes<-c('LIBA','MOBA','HEBA','LITR','MOTR','HETR','LINO','MONO','HENO',
                 'LIRA','MORA','HERA','LIWI','MOWI','HEWI','LIAI','MOAI','HEAI',
                 'LITN','MOTN','HETN','LIDT','MODT','HEDT','LITF','MOTF','HETF')

# Import the species table
cls <- read_csv("/users/alexandremacphail/desktop/commonluspp.csv") %>%
  mutate(scientific_name = paste0(species_genus, " ", species_name))

# Setup the data from the report -- change this once wt_auth() is working
data <-
  dir_ls(path = "/users/alexandremacphail/desktop/qc", regexp = "*tag_details_report.csv") %>%
  map(~read_csv(., col_types = list(location = col_character(), abundance = col_character(), verified_by = col_character()))) %>%
  bind_rows()

head(data)

```

The `wildRtrax::wt_ord` function conducts a series of steps to return the results of a multi-observer project. The first step in this process involves tidying and preparing the data in a species matrix. The `tidyverse` familyl of packages is utilized for this process, which is a collection of packages designed for efficient data manipulation and analysis. The `wt_ord` function then runs a redundancy analysis (RDA) on the observer with recording, in other words, location (spatial component) and recording date (temporal component), as a constrained effect. This statistical technique is commonly used to determine the relationship between a set of response variables and a set of predictor variables. In the context of this analysis, the RDA is used to determine the relationship between the observer and the species detections. The `wt_ord` function also conducts variance partitioning where necessary to determine the contribution of predictor variables in the RDA. This process helps to identify the factors that have the greatest influence on the relationship between the observer and the species detections. The function also returns results of a *PERMANOVA*, adjusted *R*-squared, and *F*-statistic to provide information on the overall strength and significance of the relationship of the ordination. The *PERMANOVA* is a non-parametric statistical method used to test the null hypothesis that there is no difference in the means of the groups being compared. The adjusted *R*-squared and *F*-statistic provide information on how well the model fits the data, and the significance of the differences observed between the groups.

In addition to the RDA, the wt_ord function also runs a generalized linear mixed model (GLMM) on the tag start time against observer, with recording as a random effect. GLMMs are widely used in statistical analysis, and are especially useful for analyzing data that has both fixed and random effects. By using a GLMM in this context, the wt_ord function can determine if there are any significant differences between the observers and the mean of the group.

```{r, eval = F, include = T, warning = F, message = F}
# Using the most recent quality control
res <- wt_ord(input = data, min_obs = 11, confidence = 0.67)

```

### False negative rates and BirdNET

```{r}
# Load and prepare the BirdNET data

fn <- dir_ls(path = "/users/alexandremacphail/desktop/qc", regexp = "*recording_birdnet.csv") %>%
  map(~read_csv(., col_types = list(location = col_character()))) %>%
  bind_rows()

nfn <- fn %>%
  filter(!window_start_time >= 180) %>%
  select(location, recording_date, scientific_name, window_start_time, confidence) %>%
  distinct() %>%
  inner_join(., cls %>% select(species_code, species_common_name, scientific_name), by = c("scientific_name" = "scientific_name")) %>%
  add_column(observer = "BirdNET") %>%
  add_column(project_name = "BirdNET Output") %>%
  select(project_name, location, recording_date, observer, species_code, window_start_time, confidence) %>%
  rename("tag_start_s" = 6) %>%
  mutate(tag_start_s = as.double(tag_start_s)) %>%
  add_column(abundance = "1")

data_bn <- bind_rows(data, nfn)

```

### Abundance estimation



```{r, include = T, eval=F, warning=F, message=F}


wt_replace_tmtt()


```


## Biological metrics

What is abundance, occupancy, etc.



## Occupancy modelling


## Wrangling data for analysis

```{r, include = T, eval=F, warning=F, message=F}

wt_occupancy()


```


## Constructing a classifier


```{r, include = T, eval=F, warning=F, message=F}


wt_download_tag_report()


```

<!--chapter:end:03-Acoustic-data-analysis.Rmd-->

# Camera data analysis

<!--chapter:end:04-Camera-data-analysis.Rmd-->

# Conclusions



<!--chapter:end:05-Conclusion.Rmd-->

