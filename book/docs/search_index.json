[["index.html", "Environmental sensor data management and analysis with R Preface 0.1 Who is this book for? 0.2 How should I read this book? 0.3 Impact and intentions 0.4 Acknowledgements", " Environmental sensor data management and analysis with R Alexander G. MacPhail, Marcus Becker, Elly C. Knight 2023-03-13 Preface wildRtrax (pronounced ‘wilder tracks’) is an R package to help environmental sensor data users to create full-cycle work flows from data management to analytics. 0.1 Who is this book for? This book is a valuable resource for anyone working with environmental sensor data and seeking to manage it using R. It is suitable for both experienced users and those new to the subject, as it is written in a clear and accessible style. While the book is designed to be user-friendly, it also contains advanced concepts and functions that may require additional reading to fully understand. The authors encourage readers to engage and provide feedback, as the book and workflows will continue to evolve. The content relies on understand of the following R packages: tidyverse mainly dplyr and purrr for data tidying and functional programming fs for file reading lubridate for dealing with dates and times sf for mapping spatial objects 0.2 How should I read this book? For any given environmental sensor, the general workflow is outlined below and how this book fits into the bigger picture of environmental sensor data management. Understanding the basics Collecting data Data pre-processing Data processing and verification using WildTrax Data analysis Reporting 0.3 Impact and intentions wildRtrax strives to maintain independent, scientifically credible, relevant, accessible, and transparent language and code. By doing so, we hope to provide unbiased and accurate information products that are easily understood by anyone. Our values are grounded in leadership, accuracy, ingenuity, passion, personal initiative and teamwork in the field of environmental sensor data as we seek to inspire ecologists to engage in responsible resource management. Our team values personal initiative and teamwork, and we encourage learning, feedback, to its shared success. We are passionate about our work and the impact it has on the environment, and we continuously work to fuel better products and processes through innovative ideas. You can visit the Alberta Biodiversity Monitoring Institute for more information. 0.4 Acknowledgements The Alberta Biodiversity Monitoring Institute supports and funds all operations surrounding wildRtrax and WildTrax. Many thanks to Dr. Richard Hedley for providing the basis for the wt_wac_info internal function for wac file support. "],["1-introduction.html", "Chapter 1 Introduction 1.1 Environmental sensors 1.2 Open data 1.3 Standardizing information pipelines 1.4 What is WildTrax? 1.5 Why did you build wildRtrax? 1.6 Installing the package 1.7 Usage", " Chapter 1 Introduction 1.1 Environmental sensors Environmental sensors are devices used to measure and record various environmental parameters, such as temperature, humidity, pressure, light, and sound. These sensors can be used to monitor changes in the environment over time and provide valuable information about the conditions that wildlife species experience in their habitats. In wildlife ecology, environmental sensors are commonly used to study the behavior, habitat preferences, and distribution of wildlife species. For example, temperature sensors can be used to monitor the microclimate of an animal’s habitat and determine the effects of temperature on its behavior and physiology. Humidity sensors can be used to study the impact of moisture on the distribution and abundance of species in different ecosystems. Sound sensors, principally, autonomous recording units (or ARUs) can be used to detect and identify the vocalizations of animals, allowing researchers to study communication patterns, behavior, population size, density, habitat preference, movement and many other biological metrics. Wildlife cameras, also known as trail cameras or remote cameras, capture images and videos of animals in their natural habitats, providing researchers with valuable information on behavior, population dynamics, habitat use, and other biological metrics. Environmental sensors can also be used to monitor changes in the environment caused by human activities, such as deforestation, pollution, and climate change. By measuring and recording changes in environmental conditions, researchers can better understand the impact of these activities on wildlife species and develop effective conservation strategies to protect them. 1.2 Open data Open data refers to data that is freely available for anyone to access, use, and share. Open data is important in the context of environmental sensors because it allows researchers, scientists, and policymakers to access and use data collected by sensors to gain insights into environmental conditions and make informed decisions. Environmental sensors generate large amounts of data on various environmental parameters such as audio recordings and images. By making this data openly available, researchers can analyze it to gain insights into patterns and trends over time, leading to a better understanding of environmental conditions and the impact of human activities on the environment. Open data also promotes transparency and accountability in environmental research. It allows the public to access and understand the data used to inform decisions about environmental management and conservation. Additionally, open data can lead to the development of innovative solutions and technologies that can help address environmental challenges. Open data also helps promote reproducibility ensuring independent verification of the data. Reproducibility ensures that the methods used to collect and analyze the data are transparent and can be independently verified by other researchers, ensuring that the results are reliable and accurate, establishing scientific validity. 1.3 Standardizing information pipelines Standardized workflows can help with environmental sensor data by ensuring that data is collected, processed, and analyzed in a consistent and transparent manner. This helps to ensure that the data is reliable and accurate, and can be easily compared and combined with other data sets. Standardized workflows typically involve a series of steps that are designed to ensure that data is collected and analyzed consistently across different research projects and sites. These steps may include: Pre-processing: Cleaning and filtering the raw sensor data to remove any noise or errors. Quality control: Checking the data for outliers or errors, and making corrections or adjustments as necessary. Processing: Transforming and extracting species from the raw sensor data Verification: Ensuring the species detections are correct Analysis: This step involves applying statistical or other analytical methods to the data to extract meaningful insights or calculate biological metrics. Reporting: Presenting the results of the analysis in a clear and understandable format, such as charts, graphs or tabular data. By standardizing these workflows, researchers can ensure that data is collected and analyzed consistently across different projects and organizations, reducing the risk of errors or inconsistencies. This helps to ensure that the data is reliable and accurate, and can be used to inform decisions related to environmental management and conservation. In addition, standardized workflows can facilitate data sharing and collaboration among researchers, as it ensures that data is collected and processed in a manner that is consistent with best practices and widely accepted standards. This can help to promote data integration and interoperability, enabling researchers to combine data from different sources to gain a more comprehensive understanding of environmental conditions and the impact of human activities on the environment. 1.4 What is WildTrax? WildTrax is a web-enabled portal designed to manage, store, process, share and discover environmental sensor data and the biological data extracted from the media. WildTrax was developed by the Alberta Biodiversity Monitoring Institute. wildRtrax serves as a parallel design and indicator to WildTrax for future analytics and functionalities. 1.5 Why did you build wildRtrax? By outlining a standardized and harmonized procedure for data intake, quality control, processing and verification of acoustic data from autonomous recording units (ARUs), wildRtrax and WildTrax hope to provide open work flows for using ARUs to answer meaningful biological questions in order to inform conservation and decision-making. Developing parallel-track design between an R package and a web-enabled software like WildTrax can significantly improve the software development process. It allows for simultaneous development and testing of both applications, reducing the chances of errors and incompatibilities, and formalizing data structures and APIs across a network of different applications. 1.6 Installing the package You can install wildRtrax directly from this repository with: install.packages(&quot;remotes&quot;) remotes::install_github(&quot;ABbiodiversity/wildRtrax&quot;) Or the development version with: remotes::install_github(&quot;ABbiodiversity/wildRtrax@development&quot;) 1.7 Usage All functions begin with a wt_* prefix. Column names and metadata align with the WildTrax infrastructure. The goal is to follow the work flow of pre-processing, linking with WildTrax, download and analysis. "],["2-acoustic-data-pre-processing.html", "Chapter 2 Acoustic data pre-processing 2.1 Acoustic data management basics 2.2 Reading, filtering and selecting audio recordings 2.3 Other convenience functions: distance between locations 2.4 Using acoustic indices and LDFCs 2.5 Linking acoustic data to WildTrax", " Chapter 2 Acoustic data pre-processing 2.1 Acoustic data management basics The first step after an ARU is retrieved from the field is to promptly download and secure the data to create a redundant backup. Having redundant backups not only protects against data loss but also ensures business continuity and minimizes downtime in the event of a system failure or natural disaster. Furthermore, in case there is an issue with the SD card copy during the quality control process, having a redundant backup ensures that a copy of the data can be restored. In wildRtrax and WildTrax, there are a few essential components and standards that are required in order for the data to be utilized within this context: Location: The physical, geographic place at which environmental sensors were deployed and/or biological data was collected on the landscape. This is how you would associate spatial coordinates to the raw audio data. Date and time: The temporal component of the audio recording - when the recording took place. Many time formats are possible but the recommended standard is YYYY-MM-DD Sample rate: The frequency at which audio waveforms are captured during analog-to-digital conversion in the environment. Digital audio uses pulse-code modulation and digital signals for sound reproduction, allowing signals to be stored, retrieved and transmitted without any loss of quality. Sampling rates of 44.1 kHz, 48 kHz, or 96 kHz are commonly used for capturing audio within the 20–20,000 Hz range. Higher sampling rates (&gt;96 kHz) are employed to capture ultrasonic species as needed. A recording is the raw media or audio file. The following data types are supported within wildRtrax package. They are implicitly related to how each model and type of ARU records data. See Wildlife Acoustics, Frontier BAR-LT and Audio Moth for examples to learn more. wac are proprietary, lossless compressed file formats developed by Wildlife Acoustics wav is the standard, ubiquitous uncompressed audio file format mp3 a lossy compressed audio file format; works by reducing the accuracy of certain sound components, and eliminating others flac is a lossless compressed audio file format Did you know?: When data are uploaded to WildTrax, the audio is converted and stored as flac. Some additional file name recommendations include: Omitting leading zeros for numeric delimited content, e.g. use OG-1-3-5 instead of OG-01-003-05, unless they serve a identification purpose, e.g. 3-0-A12 where -0- indicates the an absence of a treatment Metadata are also supported for *.dump, .txt, .csv, .fls, and .SM4S files Delimiters such as “-”, “_”, “@” are supported. Spaces and slashes “/” “” in location names are not supported “_000” suffixes from Wildlife Acoustics Kaleidoscope output (wac -&gt; wav conversion) are supported Audio files need to contain both spatial and temporal information which is the minimum required in order to upload media to WildTrax, e.g. ABMI-538-SW_20220506_050000, where ABMI-538-SW is the location and 20220506_050000 is the timestamp. Both these pieces of information can tell you when and where the recording took place. 2.2 Reading, filtering and selecting audio recordings Note, if you’re not an R user you can stop here and upload all of your recordings directly an organization on WildTrax. WildTrax however does not contain all the necessary data quality control measures to uptake data that are included in wildRtrax Go to the organization on WildTrax Go to Manage &gt; Upload Recordings to Organization Follow the steps in WildTrax to finalize data upload 2.2.1 Reading files The first step in data pre-processing is to determine what files exist. wt_audio_scanner is the function that provides the basis for the rest of the data pre-processing steps. The function recursively scans directories of audio data and returns standard metadata such as the file path, file name, file size, date, time, location name, that will later be used in WildTrax. files &lt;- wt_audio_scanner(path = &quot;../ABMI/ARU/ABMI-EH/2022/V1/228/228-NE&quot;, file_type = &quot;wav&quot;, extra_cols = F) &gt; head(files) # A tibble: 6 × 11 file_path size_Mb unsafe file_…¹ locat…² recording_date_time file_…³ julian year gps_e…⁴ &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; 1 /volumes/b… 3.51 Safe 228-NE… 228-NE 2021-11-21 12:35:49 wav 325 2021 NA 2 /volumes/b… 106. Safe 228-NE… 228-NE 2022-03-01 00:00:00 wav 60 2022 NA 3 /volumes/b… 31.8 Safe 228-NE… 228-NE 2022-03-01 02:00:00 wav 60 2022 NA 4 /volumes/b… 106. Safe 228-NE… 228-NE 2022-03-01 08:59:00 wav 60 2022 NA 5 /volumes/b… 31.8 Safe 228-NE… 228-NE 2022-03-01 10:29:00 wav 60 2022 NA 6 /volumes/b… 31.8 Safe 228-NE… 228-NE 2022-03-01 12:00:00 wav 60 2022 NA # … with 1 more variable: time_index &lt;int&gt;, and abbreviated variable names ¹ file_name, # ² location, ³ file_type, ⁴ gps_enabled Did you know?: Directories of acoustic data at the ABMI are organized in the following hierarchy. Organization: The institution or organization the data belongs to Sensor type: ARU Project: A project is a grouping of locations and recordings designed to answer an ecologically relevant question. Such programs at the ABMI include, Ecosystem Health (ABMI-EH), Before-After Dose Response (BADR). with location naming conventions such as ABMI-1025-SW and 2-0-A117, respectively. Deployment Year: The year the data was collected. Repeat visits of locations are defined in the year for things like trend data sets. Visit: A unique sequential key to define the year and visit the media collection took place, e.g. V1, V2, etc. Group: Is the delimited name of a location. It is aggregated in order to represent spatial relationships or clustering within the field project. (e.g. ABMI-1025-SW is the location but ABMI-1025 is the group given it defines a specific clustering of locations for the study design) Location: is the joining key to WildTrax and also the base directory to the media You can choose extra_cols = T you’ll be supplied with additional columns that can be used to help select recordings, specifically sample rate, length (seconds) and number of channels. files_extra &lt;- wt_audio_scanner(path = ../ABMI/ARU/ABMI-EH/2022/V1/228/228-NE&quot;, file_type = &quot;wav&quot;, extra_cols = T) &gt; head(files_extra) # A tibble: 6 × 14 file_path size_Mb file_…¹ locat…² recording_date_time file_…³ julian year gps_e…⁴ time_…⁵ &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;int&gt; 1 /volumes/… 3.51 228-NE… 228-NE 2021-11-21 12:35:49 wav 325 2021 NA 1 2 /volumes/… 106. 228-NE… 228-NE 2022-03-01 00:00:00 wav 60 2022 NA 1 3 /volumes/… 31.8 228-NE… 228-NE 2022-03-01 02:00:00 wav 60 2022 NA 2 4 /volumes/… 106. 228-NE… 228-NE 2022-03-01 08:59:00 wav 60 2022 NA 3 5 /volumes/… 31.8 228-NE… 228-NE 2022-03-01 10:29:00 wav 60 2022 NA 4 6 /volumes/… 31.8 228-NE… 228-NE 2022-03-01 12:00:00 wav 60 2022 NA 5 # … with 4 more variables: length_seconds &lt;dbl&gt;, sample_rate &lt;dbl&gt;, n_channels &lt;dbl&gt;, # unsafe &lt;chr&gt;, and abbreviated variable names ¹ file_name, ² location, ³ file_type, # ⁴ gps_enabled, ⁵ time_index 2.2.2 Segmenting longer files It’s possible that the recordings you have are longer than the maximum length WildTrax can currently import (1800 seconds or 320MB). In this case, you can use wt_chop to create intervals of recordings # Select a 10 minute file and chop it into 60 second segments, creating 10 x 60 second files instead of one 10-minute file wt_chop(input = files %&gt;% slice(1), segment_length = 60, output_folder = &quot;/my/output/folder&quot; wt_chop will create modulo recordings as needed; this refers to the remaining portion of the recording that is left over after it has been divided into equal intervals of the chosen duration. For example, if a recording is 120 seconds long and it is split into 50-second intervals, the modulo recording would be the final 20 seconds (e.g. 50 - 50 - 20). However, if the chosen interval duration is a factor of the total duration of the recording, there will be no modulo recording. For example, if a 120-second recording is divided into 60-second intervals, there will be no modulo recording since 60 is a factor of 120. 2.2.3 Filtering files At this stage, you can filter recordings from the wt_audio_scanner tibble output in order to select the files you’re interested in. Here’s an example of the ABMI’s Stratified Sampling Design for the Ecosystem Health Monitoring program that’s used to pick recordings across a breadth of dates and times in order to maximize the species inventory collected at a location. &gt; abmi_blocks &lt;- as_tibble(data.frame(julian = 90:210) %&gt;% + crossing(time_index = 1:4) %&gt;% + mutate(blocks = case_when(julian %in% 90:139 &amp; time_index == 1 ~ 9, # Midnights during the spring for owls + julian %in% 140:159 &amp; time_index == 1 ~ 10, # Midnights for amphibians, nocturnal breeders + julian %in% 160:179 &amp; time_index == 1 ~ 11, # Midnights for amphibians, nocturnal breeders + julian %in% 180:210 &amp; time_index == 1 ~ 12, # Midnights for amphibians, nocturnal breeders + julian %in% 90:104 &amp; time_index == 3 ~ 1, # Dawn in winter-spring for residents + julian %in% 105:119 &amp; time_index == 4 ~ 2, # Post-dawn in spring for residents, early arrivers + julian %in% 120:139 &amp; time_index == 3 ~ 3, # Dawn for early arrivers + julian %in% 140:149 &amp; time_index == 3 ~ 4, # Dawn for breeding bird chorus + julian %in% 150:159 &amp; time_index == 4 ~ 5, # Post-dawn breeding season + julian %in% 160:169 &amp; time_index == 3 ~ 6, # Dawn for breeding bird chorus + julian %in% 170:179 &amp; time_index == 4 ~ 7, # Post-dawn breeding season + julian %in% 180:210 &amp; time_index == 4 ~ 8, # Post-dawn late breeding season + TRUE ~ NA_real_), + recs = case_when(blocks %in% c(4:7) ~ 180, + TRUE ~ 60))) &gt; &gt; head(abmi_blocks) # A tibble: 6 × 4 julian time_index blocks recs &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 90 1 9 60 2 90 2 NA 60 3 90 3 1 60 4 90 4 NA 60 5 91 1 9 60 6 91 2 NA 60 The Before-After Dose-Response program is another example of a sampling design. Here, we select four recordings near dawn and one recording at dusk for each location between Julian date 140 and 210, for a total of 15 audio minutes. Again, this approach allows users to maximize species diversity while accounting for processing effort. b &lt;- wt_audio_scanner(path = &quot;../2-0-A25&quot;, file_type = &quot;wav&quot;, extra_cols = F) # A list of locations donwload from My Organizations &gt; Manage &gt; Download Locations locs &lt;- read_csv(&quot;../locations.csv&quot;) bz &lt;- b %&gt;% select(file_name:year) %&gt;% inner_join(., locs, by = c(&quot;location&quot;)) %&gt;% # Ensure time zone is correct mutate(recording_date_time = force_tz(recording_date_time, &quot;US/Mountain&quot;)) %&gt;% rowwise() %&gt;% # Get sun angle, 6 degrees before dawn defines civil twilight period mutate(angle = pull(suncalc::getSunlightPosition(date = recording_date_time, lat = latitude, lon = longitude, keep = c(&quot;altitude&quot;))) * (180/pi)) %&gt;% ungroup() %&gt;% filter(between(angle,-6,6), between(julian,140,210)) %&gt;% mutate(hour = hour(recording_date_time)) # Sample one dusk recording bz1 &lt;- bz %&gt;% filter(hour %in% c(20:22)) %&gt;% sample_n(1, replace = F) # And four dawn recordings bz2 &lt;- bz %&gt;% filter(hour %in% c(4:7)) %&gt;% sample_n(4, replace = F) # Bind the dawns and dusks together bzz &lt;- bind_rows(bz1, bz2) &gt; head(bzz) # A tibble: 5 × 17 file_name locat…¹ recording_date_time file_…² julian year latit…³ longi…⁴ eleva…⁵ buffe…⁶ &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; 1 2-0-A25_2… 2-0-A25 2021-06-26 21:20:00 wav 177 2021 54.9 -112. NA NA 2 2-0-A25_2… 2-0-A25 2021-06-29 05:38:00 wav 180 2021 54.9 -112. NA NA 3 2-0-A25_2… 2-0-A25 2021-06-27 05:37:00 wav 178 2021 54.9 -112. NA NA 4 2-0-A25_2… 2-0-A25 2021-06-26 05:36:00 wav 177 2021 54.9 -112. NA NA 5 2-0-A25_2… 2-0-A25 2021-06-30 05:39:00 wav 181 2021 54.9 -112. NA NA # … with 7 more variables: isHidden &lt;chr&gt;, trueCoordinates &lt;lgl&gt;, comments &lt;chr&gt;, # internal_wildtrax_id &lt;dbl&gt;, internal_update_ts &lt;dttm&gt;, angle &lt;dbl&gt;, hour &lt;int&gt;, and # abbreviated variable names ¹ location, ² file_type, ³ latitude, ⁴ longitude, # ⁵ elevationMeters, ⁶ bufferRadiusMeters 2.2.4 Filtering files geographically Another example of selecting files may be geographic. Here’s an example with selecting recordings with Bird Conservation Regions. library(tidyverse) library(sf) # Clean up the locations locs_next &lt;- locs %&gt;% filter(!is.na(latitude), !is.na(longitude)) %&gt;% select(location, latitude, longitude) %&gt;% distinct() %&gt;% mutate(index = row_number()) # Create a simple features object locsset &lt;- st_as_sf(locs_next %&gt;% select(location, latitude, longitude) %&gt;% distinct(), coords = c(&quot;longitude&quot;,&quot;latitude&quot;)) locsset &lt;- st_make_valid(locsset) locsset &lt;- st_set_crs(locsset, 4269) # Load a shapefile of Terrestrial BCRs of North America bcrs &lt;- read_sf(&quot;../BCR_Terrestrial.shp&quot;) # Filter to just Alberta bcrs &lt;- st_make_valid(bcrs) %&gt;% filter(PROVINCE_S == &quot;ALBERTA&quot;) # Intersect the location with the BCRs out &lt;- st_intersection(locsset, bcrs) # Join it to the original tibble locss &lt;- out %&gt;% inner_join(., locs, by = c(&quot;location&quot; = &quot;location&quot;)) %&gt;% # Now sample 10 locations per BCR group_by(BCRNAME) %&gt;% sample_n(10, replace = F) &gt; locss %&gt;% select(location, BCRNAME) Simple feature collection with 50 features and 2 fields Geometry type: POINT Dimension: XY Bounding box: xmin: -118.4339 ymin: 49.20788 xmax: -110.1006 ymax: 59.94313 Geodetic CRS: NAD83 # A tibble: 50 × 3 # Groups: BCRNAME [5] location BCRNAME geometry &lt;chr&gt; &lt;chr&gt; &lt;POINT [°]&gt; 1 270-NE BOREAL SOFTWOOD SHIELD (-111.1606 58.02175) 2 182-SE BOREAL SOFTWOOD SHIELD (-110.6437 58.5611) 3 272-NE BOREAL SOFTWOOD SHIELD (-110.4663 57.99359) 4 182-NE BOREAL SOFTWOOD SHIELD (-110.6437 58.5611) 5 242-SE BOREAL SOFTWOOD SHIELD (-110.411 58.15395) 6 154-SE BOREAL SOFTWOOD SHIELD (-110.1852 58.70439) 7 272-NW BOREAL SOFTWOOD SHIELD (-110.4663 57.99359) 8 272-SW BOREAL SOFTWOOD SHIELD (-110.4663 57.99359) 9 241-NW BOREAL SOFTWOOD SHIELD (-110.7115 58.16154) 10 270-SW BOREAL SOFTWOOD SHIELD (-111.1606 58.02175) # … with 40 more rows ggplot(bcrs) + coord_sf(crs = 4269) + geom_sf(aes(fill = BCRNAME)) + geom_sf(locss, mapping = aes()) + theme_bw() + theme(legend.position = &quot;none&quot;) + scale_fill_viridis_d() 2.3 Other convenience functions: distance between locations As ARUs are placed on the landscape by different organizations, or over time with different projects, it is possible that spatial locations may overlap. wt_location_distances takes a list of locations downloaded from WildTrax and calculates the distances between all pairs of points. Did you know?: You can go to Manage &gt; Download Locations in either a project or an organization to get a list of locations and their attributes. Remember, you need to be an organization administration to perform this operation since organizations manage locations. &gt; # Control locations from BADR LU 2 &gt; badr2 &lt;- locs %&gt;% + filter(grepl(&#39;^2-0&#39;,location)) &gt; &gt; # Big Grid 9 (Touchoowd) &gt; bu_locs &lt;- bu_locs %&gt;% + filter(grepl(&#39;^BG-9&#39;, location)) &gt; &gt; bu_badr &lt;- bind_rows(badr2, bu_locs) %&gt;% + mutate(type = case_when(grepl(&#39;^BG-&#39;,location) ~ &quot;Big Grids&quot;,TRUE ~ &quot;Before-After Dose Response&quot;)) &gt; &gt; bu_badr_map &lt;- st_as_sf(bu_badr, coords = c(&quot;longitude&quot;,&quot;latitude&quot;)) &gt; bu_badr_map &lt;- st_make_valid(bu_badr_map) &gt; bu_badr_map &lt;- st_set_crs(bu_badr_map, 4269) ggplot() + coord_sf(crs = 4269) + geom_sf(bu_badr_map, mapping = aes(colour = type)) + theme_bw() + theme(legend.position = &quot;none&quot;) + scale_fill_viridis_d() distances &lt;- wt_location_distances(input_from_tibble = bu_badr) &gt; # Filter points less then 100 meters apart &gt; distances %&gt;% + filter(distance &lt; 100) # A tibble: 26 × 3 location_from distance_to distance &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 2-0-A2 BG-9-84 42.8 2 2-0-A12 BG-9-75 8.86 3 2-0-A30 BG-9-83 74.0 4 2-0-A32 BG-9-92 17.8 5 2-0-A50 BG-9-74 5.46 6 2-0-A85 BG-9-97 93.3 7 2-0-A86 BG-9-95 2.63 8 2-0-A87 BG-9-94 34.2 9 2-0-A88 BG-9-93 34.3 10 2-0-A90 BG-9-85 16.6 # … with 16 more rows 2.3.1 Error handling and file standardization wt_audio_scanner will continue to support new file formats and file types. Error handling techniques are being put in place to identify and provide clear messages to users to ensure their data is standard to the WildTrax system before upload. File standardization helps ensure consistency and prevent errors when transferring data across different systems and applications. With these measures, wt_audio_scanner can provide a reliable and versatile tool for working with various audio file formats. Development requests can be submitted as issues to the GitHub repository. 2.4 Using acoustic indices and LDFCs Acoustic indices and long-duration false-colour (LDFC) spectrograms are two tools used in the analysis of acoustic data developed by the Towsey lab at QUT. Acoustic indices are numerical measurements that quantify various aspects of sound using different measurements that reflect the soundscpe. They are used to identify and track patterns of vocal activity over time, generate soundscape and monitor ecosystem-level patterns and changes. Some commonly known acoustic indices include the Acoustic Complexity Index (ACI), which measures the diversity of sound frequencies and amplitudes, and the Acoustic Diversity Index (ADI), which measures the number and relative abundance of different vocalizations. Long-duration false-color spectrograms are visual representations of acoustic data using index values rather than spectral generation in order to provide detailed views of sound patterns over long periods of time. The use of false-colours allows for easier interpretation of the spectrogram, as different colors can be used to represent different types of sounds. These unique spectrograms are particularly useful in studies of animal vocalizations, as they allow researchers to identify specific vocalizations and track patterns of vocal activity over long periods of time, and determine seasonal phenology patterns. wildRtrax utilizes the results of the AnalysisPrograms software which generates the csv, json and png files associated to the results, and generates tidy versions of the data that can be joined to the media in order to select recordings based on index values and patterns. Let us demonstrate some different examples while introducing the wt_run_ap and wt_glean_ap functions. wt_run_ap() wt_glean_ap() 2.4.1 Abiotic, geophonic and anthropophonic signal detection Recorder failure, damaged microphones, or recorders that are placed in an environment where prevailing winds muffle the sounds the ARU can hear, such as in alpine environments, can limit the audio quality in order to process data in an effective way. The recordings on Cirrus by default do not contain information on if the audio was defective and have a high geophonic index. On average, 12% (2015 – 2019 ABMI Ecosystem Health data) of the primary recordings that are selected at the ABMI site level can have a relatively high geophonic index or a malfunction or failure. To limit lost time and determine noise-pollution related effects on the landscape, recordings are processed regardless of anthropogenic noise, but recordings are optimized to omit as much as the geophonic bias as possible. Due to the large volume of data that is handled, it is no longer efficient to do these processes manually and these steps should be automated as much as possible. The goals are to find efficient ways to screen and filter audio data with high geophony or to flag and omit recordings that will not be used for processing. # Output the results aqd &lt;- wt_glean_ap(x = f2, input_dir = &quot;../data&quot;, purpose = &quot;abiotic&quot;) For example, noise pollution is a pervasive problem in modern society, with detrimental effects on human health and well-being, wildlife, and ecosystems. There is growing interest in the use of acoustic indices for noise mapping applications. This approach has several advantages over traditional noise mapping methods. Using acoustic sensors at 310 spatial locations in 2021 across a gradient of human footprint. We recorded sounds for 4-5 days at each location betwwen May and July calculated a range of acoustic indices. We also collected human observations of noise levels (ranked 0-3) and other noise sources (wind, rain, other noise and audio quality) using a standardized survey. We compared the acoustic indices to the human observations to determine the accuracy of the noise map generated from the indices. map_badr &lt;- wt_glean(x = badr, input_dir = &quot;../data&quot;, purpose = &quot;abiotic&quot;) 2.4.2 Biophonic signal detection We can also look at the entire seasonal phenology of an ABMI Ecosystem Health site &gt; boreal_example &lt;- wt_glean_ap(x = boreal_site, input_dir = &quot;../data&quot;) &gt; &gt; boreal_example[[2]] The great part about having a list of the .png files is that you can easily manipulate and generate new LDFCs with other attributes. &gt; boreal_example[[2]] %&gt;% + mutate(hour = lubridate::hour(recording_date_time)) %&gt;% + filter(hour == 0) %&gt;% # Look at only midnight LDFCs and acoustic index values + map_dfr(~lapply(.)) While acoustic indices are good at detecting large scale patterns, it is more difficult to detect rare and elusive signals, especially those that exceed the signal-to-noise threshold to capture the signal. It is important to use this along with spectral validation in WildTrax to determine the source of the environmental noise. 2.5 Linking acoustic data to WildTrax Wildlife Acoustics offers advanced tools for analyzing wildlife vocalizations. Two of their most popular programs are Songscope and Kaleidoscope that allows users to visualize and identify animal songs and calls and also create and run automated classifiers. The output files from these programs can be transformed into WildTrax tags to be uploaded for verification purposes. This is also the primary mechanism that ultrasonic data makes its way into the system if it is being analyzed in Kaleidoscope. 2.5.1 Sonscope results The function wt_songscope_tags reformats the output obtained from a wildlife acoustics songscope recognizer. This transformation involves converting the recognizer tags into tags that do not have a method type. This makes it possible to upload each hit as a tag in a task. &gt; wt_songscope_tags() For certain monitoring programs that deliberately deploy ARUs over extended periods to detect specific species, a high true negative rate is crucial to confirm the absence of the species when required. This typically involves recording for more extended periods of more than 30 minutes. In such cases, it is advisable to use the wt_chop function to segment the media, upload the resulting recordings to WildTrax, generate a task list, and combine it with the results of wt_songscope_tags to import all the data into WildTrax. 2.5.2 Kaleidoscope results Similarly, the function wt_kaleidoscope_tags performs the same reformatting process, but with Kaleidoscope instead. It is worth noting that this function targeted for sonic and ultrasonic species upload. wt_kaleidoscope_tags() 2.5.3 Making tasks wt_make_aru_tasks() "],["3-camera-data-pre-processing.html", "Chapter 3 Camera data pre-processing", " Chapter 3 Camera data pre-processing "],["4-acoustic-data-analysis.html", "Chapter 4 Acoustic data analysis 4.1 Biological metrics 4.2 Authorizing and downloading data from WildTrax 4.3 Wrangling data for analysis 4.4 Occupancy modelling 4.5 Constructing a classifier", " Chapter 4 Acoustic data analysis 4.1 Biological metrics What is abundance, occupancy, etc. 4.2 Authorizing and downloading data from WildTrax wt_auth() wt_get_download_summary() wt_download_report() 4.3 Wrangling data for analysis 4.3.1 Species identification and observer error Avian count surveys rely on skilled observers to identify species and estimate numbers of individuals using distinct visual or acoustic signals. Since birds rely on acoustic cues to attract mates, defend territory and communicate with one another, humans can take advantage of these unique signatures in order to identify species and individuals. In passive field-based surveys (i.e. “point counts”), human observers use distance and time-interval sampling in order for analysts to incorporate perceptibility and dectectability offsets (Buckland et al. 2001, Farnsworth et al. 2002, Solymos et al. 2013) to accurately calculate meaningful biological metrics such as population size or density which are then are used to make conservation decisions. The accuracy and reliability of these results must be high otherwise incorrect conclusions and flawed decision-making can lead to the improper management of a species or landscape. Since the advent of affordable autonomous recording unit (ARU) technology, single-visit human-based surveys have been rapidly replaced by archiveable acoustic recordings of the environment. This has established both the fields of bioacoustics and ecoacoustics, launching ARUs as the new standard for bird population monitoring. The comparison of human-based surveys and acoustic recording technology for monitoring avian populations is well-documented (Shonfield et al. 2017, Van Wilgenburg et al. 2017, Venier et al. 2017, Pankratz et al. 2017, Yip et al. 2019). Factors such as distance from the observer or ARU (Yip et al. 2016, Knight et al. 2020, Van Wilgenburg et al. 2017), frequency range and habitat type (Yip et al. 2016) all play an important role in whether a species is detected (true positive; TP) or not (true negative; TN). Researchers can leverage the acoustic recording data over point counts in order to improve data processing quality and benefit from long-term secure data storage. Ensuring the proper integration of traditional human-collected and acoustic data [REFERENCE] is key to avian monitoring management implications and conservation goals. By creating open data sets, (Docherty et al. 2021) researchers can also merge data sets together allowing for larger coproduced (Westwood et al. 2020) and reproducible analyses. False positives (FP) and false negatives (FN) in avian data sets are rarely reported in the scientific literature suggesting that they are either flawlessly collected or species are identified to a high degree of accuracy and / or precision. FPs can increase the count and therefore skew the population estimates for certain species. Morrison (2016) proposes that to avoid observer error in plant identification, additional training including active feedback approaches, continual evaluation and calibration among a multi-observer set are important. Similarly, Dennett and Nielsen (2019) determined that abundance and scale of the survey were the most influential factors in detecting rare species of plants. These types of assessments would be operationally difficult or costly to implement for field-based avian surveys given the mobile nature of birds, however utilizing the ability to return to acoustic recording would show the potential to determine the error in the avian count data. Standardizing multiple observers is known to be effective for correcting inter-observer variability in species detections in a monitoring program [REFERENCE]. While acoustic data excludes any visual cues for species identification, the principal advantage of collecting avian data with ARUs is that a permanent copy of the recording can be kept for future reference. A standardized system of reporting, assessing and determining actions to resolve avian identification error is crucial to ensure that reporting meets a high data quality standard and that it can be used comfortably in aggregation with other data to answer a wider range of scientific questions. The goals were to provide a standard way of assessing the prevalence of false positives, false negatives and true negatives in avian datasets, to provide solutions to correct and integrate identification error into analyses, and to provide recommendations for the evaluation, training and development of skilled observers. # Omit any abiotic data abiotic_codes&lt;-c(&#39;LIBA&#39;,&#39;MOBA&#39;,&#39;HEBA&#39;,&#39;LITR&#39;,&#39;MOTR&#39;,&#39;HETR&#39;,&#39;LINO&#39;,&#39;MONO&#39;,&#39;HENO&#39;, &#39;LIRA&#39;,&#39;MORA&#39;,&#39;HERA&#39;,&#39;LIWI&#39;,&#39;MOWI&#39;,&#39;HEWI&#39;,&#39;LIAI&#39;,&#39;MOAI&#39;,&#39;HEAI&#39;, &#39;LITN&#39;,&#39;MOTN&#39;,&#39;HETN&#39;,&#39;LIDT&#39;,&#39;MODT&#39;,&#39;HEDT&#39;,&#39;LITF&#39;,&#39;MOTF&#39;,&#39;HETF&#39;) # Import the species table cls &lt;- read_csv(&quot;/users/alexandremacphail/desktop/commonluspp.csv&quot;) %&gt;% mutate(scientific_name = paste0(species_genus, &quot; &quot;, species_name)) # Setup the data from the report -- change this once wt_auth() is working data &lt;- dir_ls(path = &quot;/users/alexandremacphail/desktop/qc&quot;, regexp = &quot;*tag_details_report.csv&quot;) %&gt;% map(~read_csv(., col_types = list(location = col_character(), abundance = col_character(), verified_by = col_character()))) %&gt;% bind_rows() head(data) The wildRtrax::wt_ord function conducts a series of steps to return the results of a multi-observer project. The first step in this process involves tidying and preparing the data in a species matrix. The tidyverse familyl of packages is utilized for this process, which is a collection of packages designed for efficient data manipulation and analysis. The wt_ord function then runs a redundancy analysis (RDA) on the observer with recording, in other words, location (spatial component) and recording date (temporal component), as a constrained effect. This statistical technique is commonly used to determine the relationship between a set of response variables and a set of predictor variables. In the context of this analysis, the RDA is used to determine the relationship between the observer and the species detections. The wt_ord function also conducts variance partitioning where necessary to determine the contribution of predictor variables in the RDA. This process helps to identify the factors that have the greatest influence on the relationship between the observer and the species detections. The function also returns results of a PERMANOVA, adjusted R-squared, and F-statistic to provide information on the overall strength and significance of the relationship of the ordination. The PERMANOVA is a non-parametric statistical method used to test the null hypothesis that there is no difference in the means of the groups being compared. The adjusted R-squared and F-statistic provide information on how well the model fits the data, and the significance of the differences observed between the groups. In addition to the RDA, the wt_ord function also runs a generalized linear mixed model (GLMM) on the tag start time against observer, with recording as a random effect. GLMMs are widely used in statistical analysis, and are especially useful for analyzing data that has both fixed and random effects. By using a GLMM in this context, the wt_ord function can determine if there are any significant differences between the observers and the mean of the group. # Using the most recent quality control res &lt;- wt_ord(input = data, min_obs = 11, confidence = 0.67) 4.3.2 False negative rates and BirdNET # Load and prepare the BirdNET data fn &lt;- dir_ls(path = &quot;/users/alexandremacphail/desktop/qc&quot;, regexp = &quot;*recording_birdnet.csv&quot;) %&gt;% map(~read_csv(., col_types = list(location = col_character()))) %&gt;% bind_rows() nfn &lt;- fn %&gt;% filter(!window_start_time &gt;= 180) %&gt;% select(location, recording_date, scientific_name, window_start_time, confidence) %&gt;% distinct() %&gt;% inner_join(., cls %&gt;% select(species_code, species_common_name, scientific_name), by = c(&quot;scientific_name&quot; = &quot;scientific_name&quot;)) %&gt;% add_column(observer = &quot;BirdNET&quot;) %&gt;% add_column(project_name = &quot;BirdNET Output&quot;) %&gt;% select(project_name, location, recording_date, observer, species_code, window_start_time, confidence) %&gt;% rename(&quot;tag_start_s&quot; = 6) %&gt;% mutate(tag_start_s = as.double(tag_start_s)) %&gt;% add_column(abundance = &quot;1&quot;) data_bn &lt;- bind_rows(data, nfn) 4.3.3 Abundance estimation wt_replace_tmtt() 4.4 Occupancy modelling wt_occupancy() 4.5 Constructing a classifier wt_download_tag_report() "],["5-camera-data-analysis.html", "Chapter 5 Camera data analysis", " Chapter 5 Camera data analysis "],["6-conclusions.html", "Chapter 6 Conclusions", " Chapter 6 Conclusions "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
