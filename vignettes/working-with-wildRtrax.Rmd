---
title: "working-with-wildRtrax"
output: rmarkdown::html_vignette
author: 
- Alex MacPhail^[Alberta Biodiversity Monitoring Institute, agmacpha@ualberta.ca]
vignette: >
  %\VignetteIndexEntry{working-with-wildRtrax}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
runtime: shiny
bibliography: S1877050914002403.bib
link-citations: yes

---

```{r, include = FALSE, echo = FALSE}

knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

library(shiny)
library(vroom)
library(reticulate)
library(vegan)
library(lubridate)
library(pipeR)
library(tidyverse)
library(bioacoustics)
library(tuneR)
library(tictoc)
library(furrr)
library(seewave)
library(magick)

blanktheme<-theme_bw() +
    theme(panel.background = element_blank(), 
      panel.grid.major = element_blank(), 
      panel.grid.minor = element_blank()) #A blank theme for graphs

```

```{r setup, include = FALSE, eval = FALSE, echo = FALSE}

devtools::install_github("ABbiodiversity/wildRtrax")

```


This document contains an outline for how to use the functions from the R package **wildRtrax** for environmental sensor data management and analytics. 

## **FAQ**

### What is **wildRtrax**?

**wildRtrax**, pronounced *'wilder tracks'*, is an R package for ecologists and advanced users who work with environmental sensor data, mainly from acoustic recordings units (ARUs) or remote game cameras (RGCs). It contains functions designed to meet most needs in order to organize, analyze and standardize data. **wildRtrax** is self-contained and must be run under an R statistical environment, and it also depends on many other R packages. **wildRtrax** is free software and distributed under [MIT License (c) 2020](https://github.com/ABbiodiversity/wildRtrax/blob/master/LICENSE). All package functions are designed to standardize data to the [WildTrax](https://wwww.wildtrax.ca) infrastructure.

### What is **WildTrax**?

[**WildTrax**](https://www.wildtrax.ca) is a web-enabled portal to centralize, store, manage and share environmental sensor data. It was developed by the [Alberta Biodiversity Monitoring Institute](https://abmi.ca) and the [Bioacoustic Unit](https://bioacoustic.abmi.ca). **wildRtrax** serves as a parallel design and indicator to analytics and functions that can be developed in **WildTrax**. 

### What R packages does **wildRtrax** depend on?

**wildRtrax** depends on a multitude of packages to provide flexible routines and work flows for data management. [`tidyverse`](https://tidyverse.org) for piping functions, standard grammar and tidy data manipulation, [`furrr`](https://davisvaughan.github.io/furrr/) and [`doParallel`](https://cran.r-project.org/web/packages/doParallel/index.html) for parallel computing, and acoustic analysis packages: [`bioacoustics`](https://cran.r-project.org/web/packages/bioacoustics/index.html), [`tuneR`](https://cran.r-project.org/web/packages/tuneR/index.html), [`seewave`](https://cran.r-project.org/web/packages/seewave/seewave.pdf). Certain functions are indebted to the [QUT Ecoacoustics Analysis Software](https://github.com/QutEcoacoustics/audio-analysis)

### Do I need to know any other programming langugages in order to use **wildRtrax**?

Certain functions rely on Python and bash scripts and some SQL. You do not need to know the languages in order to use the functions but it is encouraged to read the documentation.

### How do I report a bug in **wildRtrax**?

If you think you have found a bug in **wildRtrax**, you should report it to developers or maintainers. Please do not send bug reports to R mailing lists, since **wildRtrax** is not a standard R package. The preferred forum to report bugs is [GitHub](https://github.com/ABbiodiversity/wildRtrax/issues).  Here is what is required in order to report a bug - issues are welcome and encouraged and are the only way to make **wildRtrax** non-buggy:

* The bug report should be so detailed that the bug can be replicated and corrected
* Send an example that causes a bug
* Send a minimal data set as well if it is not available in R
* Paste the output or error message in your message
* Specify which version of **wildRtrax** you used

### Can I contribute to **wildRtrax**?

Yes! **wildRtrax** is dependent on user contribution and all feedback is welcome. If you have problems with **wildRtrax**, it may be as simple as incomplete documentation. Feature requests also are welcome, but they are not necessarily fulfilled. A new feature will be added if it is easy to do and it looks useful to the user base of the package, or if you submit fully annotated code.



------------------------------------------------------------------------

## **Introduction**

Autonomous recording units (ARUs) and remote game cameras (RGCs) collect data on the environment by means of capturing acoustic or visual data, respectively. ARUs are used to survey a variety of species such as birds, amphibians, and bats. But really anything that gives a vocalizing cue. RGCs are best used to detect and monitor mammals. Check out [`abmi.camera.extras`](https://github.com/mabecker89/abmi.camera.extras) if you're interested in getting estimates of animal density collected from RGCs.

Both environmental sensors are designed to record sound or images autonomously for long periods of time. This is turn can accrue a large amount of data. 

### **Audio file formats**

There are four major audio file types used within the **wildRtrax** framework: *wac*, *wav*, *mp3* and *flac*

* *wac, w4v* are proprietary, lossless compressed file formats developed by [Wildlife Acoustics](https://www.wildlifeacoustics.com/)
* *wav* is the standard, ubiquitous uncompressed audio file format
* *mp3* a lossy compressed audio file format; works by reducing the accuracy of certain sound components, and eliminating others
* *flac* a lossless compressed format useful for efficiently packing audio data

Let's scan through a folder of audio data:

```{r Audio files, include = TRUE}

audio_files <-
  # First list then retrieve file size from a given directory
  fs::dir_ls(path = '/users/alexandremacphail/desktop/testwav',
         recurse = TRUE,
         regexp = '\\.wac$|\\.w4v$|\\.wav$|\\.mp3$|\\.flac$') %>>%
  "Scanning audio files ..." %>>%
  furrr::future_map_dbl(., .f = ~ fs::file_size(.), .progress = TRUE, .options = furrr_options(seed = TRUE)) %>%
  tibble::enframe() %>%
  dplyr::mutate(
    size = round(value / 10e5, digits = 2),   # Convert size to megabytes
    filename = basename(name),
    filetype = as_factor(tools::file_ext(filename)),
    compressed = dplyr::case_when(filetype %in% c('wac', 'mp3', 'flac', 'w4v') ~ TRUE, TRUE ~ FALSE)) %>%
  dplyr::select(name, filename, size, filetype, compressed)
  
```


```{r Figure 1 - Audio file formats, fig.align='center', out.extra='angle=90', fig.width = 7, fig.height = 5, echo=FALSE, include = T}

ggplot2::ggplot(audio_files, aes(x = reorder(filetype, size) , y = size, fill = compressed)) + geom_bar(stat="identity") + blanktheme + coord_flip() + xlab("Filetype") + ylab("Size (MB)")

```

Even using a high bit rate (b = 320) for the mp3 files, the reduction in size between formats is significant. Nevertheless, loss of information in the audio data is an important consideration to make when converting or analyzing audio files. mp3 files that are stored in WildTrax for transcription, for example, truncate all acoustic data above 12 kHz. This is an important consideration to make when studying a species of interest - ensure the *sample rate (x2 for stereo)* and *species frequency range* match.

It is important to understand how an R environment deals with audio data as well. wav files are ubiquitously used and compatible for editing and manipulation as the uncompressed format provides a raw standard. The `tuneR` package has functions that are used to read, write and manipulate acoustic recordings. The `readWave` function and the `header` argument can be used to either store information in R as the `S4 wave object` proper, or a list with the audio metadata.

```{r Examples of reading acoustic data into R, include = TRUE, echo = TRUE}

file <- c('/users/alexandremacphail/desktop/testwav/ABMI-0439-NW_20190530_054400.wav')

wave_t <- tuneR::readWave(file, header = T) #True header format

wave_f <- tuneR::readWave(file, header = F)

list(wave_t, wave_f)

```

You can access the *objects* in `wave_t` through a `$` as you would a normal list. When `header = FALSE` and you are reading in the entire wav file, you can access *slots* of the `S4` object using `@`. 

```{r Accessing S4 and lists, include = TRUE, echo = TRUE}

sound_length_S4 <- round((wave_f@left / wave_f@samp.rate), 2)

#Is equivalent to:

sound_length_list <- wave_t$samples / wave_t$sample.rate

```

When it comes to different file formats `read_wac`, `wav2flac`, `readMP3` and `read_audio` are functions that all converts these format to an S4 wave object. 

```{r, include = TRUE, echo = TRUE}

file_wac <- c('/users/alexandremacphail/desktop/testwav/ABMI-0439-NE_0+1_20190526_054900.wac')

file_flac <- c('/users/alexandremacphail/desktop/testwav/ABMI-0439-NW_0+1_20190530_054400.flac')

wac <- read_wac(file_wac)

flac <- wav2flac(file_flac, reverse = TRUE) #If needing to convert to a flac use reverse = FALSE

```

### **Spectrograms**

A *spectrogram* is a visual representation of the spectrum of frequencies of an audio signal as it varies with time ([Wikipedia](https://en.wikipedia.org/wiki/Spectrogram)). Spectrograms can be used to identify animal vocalizations by their unique spectral signature. Generally speaking, there are three pieces of information you can extract from a spectrogram:

* Length of the vocalization via the x-axis in seconds
* Frequency range of the vocalization via the y-axis in Hz
* Relative amplitude of the vocalization via the z-axis in dBFS ([*decibels relative to full scale*](https://en.wikipedia.org/wiki/DBFS))

Let's create a spectrogram to get a better look at some of those audio files. Here's one way to do it using `ggspectro` in `seewave`. 

```{r Figure 2 - Create a spectrogram, fig.align='center', out.extra='angle=90', fig.width = 7, fig.height = 5, warnings = FALSE, include = TRUE}

audio_files <- audio_files %>%
  dplyr::filter(filename %in% c('ABMI-0439-NW_20190530_054400.wav','ABMI-0439-NW_20190529_054500.wav'))

#You can use something like a loop and ggpspectro from seewave
for (i in 1:nrow(audio_files)) {
  tryCatch(
      t <- tuneR::readWave(audio_files$name[i], from = 0, to = 60, units ="seconds"),
      error = function(e) {
        msg <- conditionMessage(e)
        print(paste0(msg, i, sep=' *** '))}
    )
  v <- seewave::ggspectro(t, ovlp = 50) + ggplot2::geom_tile(aes(fill=amplitude)) + labs(title = audio_files$filename[i]) + blanktheme
  print(v)
}

```

[**SoX**](http://sox.sourceforge.net/) is also a very powerful command line tool that can build spectrograms as well. Processing time here is much faster given R doesn't have to read in the file as an S4 wave object. More on this later. 

```{r SoX images in bash, engine='bash', include = TRUE}

#Or try a bash command using SoX
cd /users/alexandremacphail/desktop/testwav && for file in *.wav; do outfile="${file%.*}.png"; title_in_pic="${file%.*}"; sox "$file" -n spectrogram -l -m -t "$title_in_pic" -o "$outfile"; done

```

You'll notice that we are not manipulating any of the features of the wav file - simply reading in the raw acoustic information. You can read the images back into R using and create an array of images. See [`magick`](https://cran.r-project.org/web/packages/magick/index.html) for more functionality. 

```{r Read SoX images, fig.width = 5, fig.height = 3, fig.align = 'center', include = TRUE}

pics <- list.files("/users/alexandremacphail/desktop/testwav",".png", recursive = T, full.names = T)
images <- image_read(pics)
image_append(image_border(images, "black", "2x2"), stack = TRUE)

```

Manipulating spectral signatures... 

```{r Manipulating spectral signatures, include = TRUE}

#Content coming soon!

```

### **Photo file formats**

Content coming soon!

--------------------------------------

## **Working with messy data: an introduction to environmental sensor data management**

### **From the field to the office**

Familiarity with processes, protocols, equipment and data is an important first step in understanding how to manage environmental sensor data. Check with your study design or monitoring plan to ensure that you are correctly managing your data prior to heading into the field. **wildRtrax** doesn't focus on the field components of the data flows but is heavily dependent on it. Acoustic data has certain metadata dependencies that can be extracted from certain aspects of the raw data, but robust field or *visit* metadata as it's called in **wildRtrax** and **WildTrax** is important to support the quality control process of the incoming sensor data.

### **Metadata dependencies for acoustic data**

You'll notice in the `audio_files` tibble, there is the *file name* column. The **wildRtrax** framework prefers that the file name string from which the data is deriving is composed of two parts: a *spatial* component and a *temporal* component. We call these fields the *location* and the *recording_date_time* of the recording, respectively. The location, date and time should be critical pieces of information that should be collected and checked when you are visiting environmental sensors in the field. 

* **Location**
  * Is a string (or character vector) identifying a unique spatial location of your study
  * The following is currently supported:
    * '^(.*?)(?:_+0\+1_?|_+0_?)?[_-](\d{12,14})(\.\w{2,6})?$' *or*
    * '^(.*?)(?:_+0\+1_?|_+0_?)?[_-](\d{8})[-_\$](\d{4,6})(\.\w{2,6})?$' *or* 
    * '^(.*?)(?:_+0\+1_?|_+0_?)?[_-](\d{4})[_-](\d{2})[_-](\d{2})[_-\$](\d{2})[_-:](\d{2})[_-:]?(\d{0,2})(\.\w{2,6})?$'

* **Date time**
  * Contains a POSIX.ct format date time

Let's expand on the `audio_files` tibble a bit more to extract this information:

```{r Adding spatiotemporal metadata, include = TRUE}

audio_files <-
  fs::dir_ls(path = "/volumes/budata/abmi/2019/01/abmi-0439/abmi-0439-ne",
                   recurse = TRUE,
                   regexp = '\\.wac$|\\.wav$') %>>%
    "Scanning audio files in path ..." %>>%
    furrr::future_map_dbl(., .f = ~ fs::file_size(.), .progress = TRUE, .options = future_options(seed = TRUE)) %>%
    tibble::enframe() %>%
    # Convert file sizes to megabytes
    dplyr::mutate(size_Mb = round(value / 10e5, digits = 2)) %>%
    dplyr::select(file_path = name, size_Mb) %>%
    dplyr::mutate(file_name = stringr::str_replace(basename(file_path), "\\..*", "")) %>%
    dplyr::mutate(file_type = tools::file_ext(file_path)) %>%
    # Parse location and recording date time
    tidyr::separate(
      file_name,
      into = c("location", "recording_date_time"),
      sep = "(?:_0\\+1_|_|__0__|__1__)", #Strips Wildlife Acoustics SM3 file naming convention for channels
      extra = "merge",
      remove = FALSE
    ) %>%
    # Create date/time fields
    dplyr::mutate(
      recording_date_time = lubridate::ymd_hms(recording_date_time),
      julian = lubridate::yday(recording_date_time),
      year = lubridate::year(recording_date_time))

audio_files

```

Great! So now we know when and where all the audio data was from. 

Another important aspect of collecting long-duration acoustic data is ensuring that the number of recordings that were planned on being collected are in fact there. Wildlife Acoustics has a proprietary formatting mechanism called a *SET file* that can be used to program their ARUs to a set schedule. This functionality is not transferable between different makes of ARUs, so a standard needs to be established. Given that each make and model will have a way of programming this schedule, we can simple index it by the spatial and temporal information. 

Let's give our pipe an indexing function and a larger acoustic data set to work with...

```{r Add time indexing, include = TRUE} 

tic()#And a timer to see how long things take!

audio_files <- 
  fs::dir_ls(path = "/volumes/budata/abmi/2019/01/abmi-0439/abmi-0439-ne",
                   recurse = TRUE,
                   regexp = '\\.wac$|\\.wav$') %>>%
    "Scanning audio files in path ..." %>>%
    furrr::future_map_dbl(., .f = ~ fs::file_size(.), .progress = TRUE, .options = future_options(seed = TRUE)) %>%
    tibble::enframe() %>%
    # Convert file sizes to megabytes
    dplyr::mutate(size_Mb = round(value / 10e5, digits = 2)) %>%
    dplyr::select(file_path = name, size_Mb) %>%
    dplyr::mutate(file_name = stringr::str_replace(basename(file_path), "\\..*", "")) %>%
    dplyr::mutate(file_type = tools::file_ext(file_path)) %>%
    # Parse location and recording date time
    tidyr::separate(
      file_name,
      into = c("location", "recording_date_time"),
      sep = "(?:_0\\+1_|_|__0__|__1__)", #Strips Wildlife Acoustics SM3 file naming convention for channels
      extra = "merge",
      remove = FALSE
    ) %>%
    # Create date/time fields
    dplyr::mutate(
      recording_date_time = lubridate::ymd_hms(recording_date_time),
      julian = lubridate::yday(recording_date_time),
      year = lubridate::year(recording_date_time)) %>%
    dplyr::arrange(location, recording_date_time) %>%
    # Create time index
    dplyr::group_by(location, year, julian) %>%
    dplyr::mutate(time_index = dplyr::row_number()) %>%
    dplyr::ungroup()

sum(audio_files$size_Mb)/1000 #Total size of directory in GB

toc()

```

Looks like we are recording eight times a day throughout various times of day. But this is just one location of data and we already had to scan through ~30GB of wac data. If these were wav, it would be ~60GB. Monitoring programs usually rely on collection of data from a multitude of sensors. This is where data compression comes in handy, but when you scale to the level of a multi-year monitoring program you can't avoid the data accumulation issue. See **Case studies** for more details.

```{r, echo = TRUE, include=FALSE, fig.align='center'}

server <- function(input, output) {
  sliderValues <-
    reactive({
      (
        input$variable1 * input$variable4 * (input$variable3 * 16 * 2 * input$variable2 *
                                               60) / 8
      ) * 0.000000000001
    })
  pl <- reactive({
    sliderValues()
  })
  output$values <-
    renderText({
      paste0("Data accumulated: ", sliderValues(), " TB")
    })
}

ui <- pageWithSidebar(
  headerPanel("Data accumulation"),
  sidebarPanel(
    sliderInput(
      "variable1",
      "Days:",
      min = 0,
      max = 365,
      value = 1
    ),
    sliderInput(
      "variable2",
      "Audio minutes recorded per day:",
      min = 0,
      max = 1440,
      value = 10
    ),
    sliderInput(
      "variable3",
      "Sample rate:",
      min = 0,
      max = 48000,
      value = 44100
    ),
    sliderInput(
      "variable4",
      "Locations surveyed:",
      min = 0,
      max = 1000,
      value = 1
    )
  ),
  mainPanel(tableOutput("values"))
)

shinyApp(ui, server)

```

### **Data volume, storage and computing power**

OK so we know we're going to accrue a lot of data using environmental sensors. Are there some ways we can reduce this?

* *Are you interested in one or many species?*
  * A community analysis would require a broad spectrum range to record in or analyze or with more data being collected to account for imperfect detection. Whereas with a single or multi-species approach, you may only need to look at a narrow frequency range in order to detect the species. 
    * Show example with busy dawn recording vs BCFR CI3
* *What frequency range does your species vocalize in?*
  * If you know the frequency range your species vocalizes in, you may be able to *change the sampling rate*, apply a *band-pass filter* or *compress the data*
    * Show what changing the sampling rate does
    * Show what a band-pass filter does
    * Show what compressing data does
* *Data quality or data volume?*
  * All methods above will inherently reduce data quality in favour of also reducing data volume. With more complex acoustic information comes larger files.
    * Show size of compressed audio files between dawn summer and winter nights (extremes)
  
What if none of the above are acceptable? Are there other ways we can overcome this big data hurdle?

You might already be aware that **wildRtrax** and some of the functions previously described rely on *parallel computing* in order to speed up function run times. In general, the more cores you have, the faster you can run operations. For example `future_map_dbl` is a parallel computing function based off of `purrr`s `map_dbl`. See also its `pmap_dbl` cousin.

### Scanning through data: `wt_audio_scanner`

Let's look at the output of the first `wildRtrax` function based on what we know now

```{r Running wt_audio_scanner}

#library(wildRtrax)

#audio_files <- wt_audio_scanner(path = "/volumes/budata/abmi/2019/01/abmi-0439", file_type = "both")

```

### Assessing the situation

#### Scenario 1: Detection of ARU failures



#### Scenario 2:

#### Scenario 3:

### Resolving data structure issues: *Python functions*

```{r}

```

### Summarizing results

------------------------------------------------------------------------

## **Working with a clean data set: pushing to WildTrax**

### Scanning and subsetting data using `wt_audio_scanner`

Given that there are thousands are recordings for each location, we'll need to select which recordings will be processed by a human. 

```{r Subsetting data options using wt_audio_scanner}



```

### Cleaning up recordings with `wt_varnish`

Now that we have our data selected, we also want to ensure that the recordings will be the correct length before they go into WildTrax. You can do this by either choosing the *method*, However sometimes situations may occur where you 

### Linking data for quick upload with `wt_link`

------------------------------------------------------------------------

## **Advanced audio data stuff**

### Acoustic indices

Acoustic indices are statistics that summarize an aspect of the distribution of acoustic energy from an audio recording. There are excellent resources for the science behind these here: [@article]. 

### Long-duration false-colour spectrograms

### `wt_run_ap` and `wt_blow`

### Other options for acoustic index extraction

------------------------------------------------------------------------

## ****

To note that you also have the option to upload your entire acoustic data set to **WildTrax** 

### Study design case study 1: 

### Study design case study 2: Seasonal phenology

### Study design case study 3: Multi-observer record analysis with `wt_carrefour`

------------------------------------------------------------------------

## **Future directions**

------------------------------------------------------------------------

# References


